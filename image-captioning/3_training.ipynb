{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will train your CNN-RNN model.  \n",
    "\n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train your Model\n",
    "- [Step 3](#step3): (Optional) Validate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.88s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 349/414113 [00:00<01:58, 3484.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [01:35<00:00, 4333.81it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 32         # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 256           # dimensionality of image and word embeddings\n",
    "hidden_size = 256          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(encoder.embed.parameters()) + list(decoder.parameters())\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/12942], Loss: 4.1130, Perplexity: 61.13018\n",
      "Epoch [1/3], Step [200/12942], Loss: 4.0911, Perplexity: 59.80310\n",
      "Epoch [1/3], Step [300/12942], Loss: 4.0342, Perplexity: 56.49571\n",
      "Epoch [1/3], Step [400/12942], Loss: 3.7795, Perplexity: 43.7958\n",
      "Epoch [1/3], Step [500/12942], Loss: 3.8185, Perplexity: 45.5380\n",
      "Epoch [1/3], Step [600/12942], Loss: 3.3603, Perplexity: 28.7989\n",
      "Epoch [1/3], Step [700/12942], Loss: 3.3530, Perplexity: 28.5891\n",
      "Epoch [1/3], Step [800/12942], Loss: 3.3921, Perplexity: 29.7271\n",
      "Epoch [1/3], Step [900/12942], Loss: 3.4377, Perplexity: 31.1150\n",
      "Epoch [1/3], Step [1000/12942], Loss: 3.1114, Perplexity: 22.4527\n",
      "Epoch [1/3], Step [1100/12942], Loss: 3.0572, Perplexity: 21.26694\n",
      "Epoch [1/3], Step [1200/12942], Loss: 2.8693, Perplexity: 17.6241\n",
      "Epoch [1/3], Step [1300/12942], Loss: 3.5355, Perplexity: 34.3110\n",
      "Epoch [1/3], Step [1400/12942], Loss: 2.7739, Perplexity: 16.0214\n",
      "Epoch [1/3], Step [1500/12942], Loss: 2.8804, Perplexity: 17.8220\n",
      "Epoch [1/3], Step [1600/12942], Loss: 3.1154, Perplexity: 22.5418\n",
      "Epoch [1/3], Step [1700/12942], Loss: 3.2978, Perplexity: 27.0528\n",
      "Epoch [1/3], Step [1800/12942], Loss: 2.7095, Perplexity: 15.0223\n",
      "Epoch [1/3], Step [1900/12942], Loss: 2.9341, Perplexity: 18.80485\n",
      "Epoch [1/3], Step [2000/12942], Loss: 2.6838, Perplexity: 14.6409\n",
      "Epoch [1/3], Step [2100/12942], Loss: 2.8289, Perplexity: 16.9264\n",
      "Epoch [1/3], Step [2200/12942], Loss: 3.2176, Perplexity: 24.9678\n",
      "Epoch [1/3], Step [2300/12942], Loss: 2.9336, Perplexity: 18.7954\n",
      "Epoch [1/3], Step [2400/12942], Loss: 3.1653, Perplexity: 23.6965\n",
      "Epoch [1/3], Step [2500/12942], Loss: 2.8407, Perplexity: 17.1281\n",
      "Epoch [1/3], Step [2600/12942], Loss: 3.0153, Perplexity: 20.3960\n",
      "Epoch [1/3], Step [2700/12942], Loss: 2.5402, Perplexity: 12.6818\n",
      "Epoch [1/3], Step [2800/12942], Loss: 2.8390, Perplexity: 17.0978\n",
      "Epoch [1/3], Step [2900/12942], Loss: 2.9546, Perplexity: 19.1947\n",
      "Epoch [1/3], Step [3000/12942], Loss: 2.6080, Perplexity: 13.5721\n",
      "Epoch [1/3], Step [3100/12942], Loss: 2.6228, Perplexity: 13.7746\n",
      "Epoch [1/3], Step [3200/12942], Loss: 2.6611, Perplexity: 14.3123\n",
      "Epoch [1/3], Step [3300/12942], Loss: 2.4965, Perplexity: 12.1395\n",
      "Epoch [1/3], Step [3400/12942], Loss: 2.3659, Perplexity: 10.6539\n",
      "Epoch [1/3], Step [3500/12942], Loss: 2.6758, Perplexity: 14.5245\n",
      "Epoch [1/3], Step [3600/12942], Loss: 2.1743, Perplexity: 8.79638\n",
      "Epoch [1/3], Step [3700/12942], Loss: 2.5138, Perplexity: 12.3521\n",
      "Epoch [1/3], Step [3800/12942], Loss: 2.7181, Perplexity: 15.1522\n",
      "Epoch [1/3], Step [3900/12942], Loss: 2.6127, Perplexity: 13.6363\n",
      "Epoch [1/3], Step [4000/12942], Loss: 3.0079, Perplexity: 20.2444\n",
      "Epoch [1/3], Step [4100/12942], Loss: 2.6014, Perplexity: 13.4823\n",
      "Epoch [1/3], Step [4200/12942], Loss: 2.4850, Perplexity: 12.0015\n",
      "Epoch [1/3], Step [4300/12942], Loss: 2.3037, Perplexity: 10.0107\n",
      "Epoch [1/3], Step [4400/12942], Loss: 2.7223, Perplexity: 15.2147\n",
      "Epoch [1/3], Step [4500/12942], Loss: 2.6745, Perplexity: 14.5047\n",
      "Epoch [1/3], Step [4600/12942], Loss: 2.6983, Perplexity: 14.8549\n",
      "Epoch [1/3], Step [4700/12942], Loss: 2.8459, Perplexity: 17.2164\n",
      "Epoch [1/3], Step [4800/12942], Loss: 2.6533, Perplexity: 14.2011\n",
      "Epoch [1/3], Step [4900/12942], Loss: 2.6061, Perplexity: 13.5463\n",
      "Epoch [1/3], Step [5000/12942], Loss: 2.2968, Perplexity: 9.94221\n",
      "Epoch [1/3], Step [5100/12942], Loss: 2.5099, Perplexity: 12.3039\n",
      "Epoch [1/3], Step [5200/12942], Loss: 2.2352, Perplexity: 9.34870\n",
      "Epoch [1/3], Step [5300/12942], Loss: 2.3240, Perplexity: 10.2160\n",
      "Epoch [1/3], Step [5400/12942], Loss: 2.9322, Perplexity: 18.7695\n",
      "Epoch [1/3], Step [5500/12942], Loss: 2.8382, Perplexity: 17.0854\n",
      "Epoch [1/3], Step [5600/12942], Loss: 2.3443, Perplexity: 10.4257\n",
      "Epoch [1/3], Step [5700/12942], Loss: 2.4535, Perplexity: 11.6288\n",
      "Epoch [1/3], Step [5800/12942], Loss: 2.4655, Perplexity: 11.7692\n",
      "Epoch [1/3], Step [5900/12942], Loss: 2.3613, Perplexity: 10.6047\n",
      "Epoch [1/3], Step [6000/12942], Loss: 2.4126, Perplexity: 11.1629\n",
      "Epoch [1/3], Step [6100/12942], Loss: 2.5717, Perplexity: 13.0877\n",
      "Epoch [1/3], Step [6200/12942], Loss: 2.6619, Perplexity: 14.3235\n",
      "Epoch [1/3], Step [6300/12942], Loss: 3.0717, Perplexity: 21.57954\n",
      "Epoch [1/3], Step [6400/12942], Loss: 2.1992, Perplexity: 9.01811\n",
      "Epoch [1/3], Step [6500/12942], Loss: 2.5052, Perplexity: 12.2460\n",
      "Epoch [1/3], Step [6600/12942], Loss: 2.2953, Perplexity: 9.92766\n",
      "Epoch [1/3], Step [6700/12942], Loss: 2.4560, Perplexity: 11.6586\n",
      "Epoch [1/3], Step [6800/12942], Loss: 2.2502, Perplexity: 9.48984\n",
      "Epoch [1/3], Step [6900/12942], Loss: 2.7813, Perplexity: 16.1395\n",
      "Epoch [1/3], Step [7000/12942], Loss: 2.5335, Perplexity: 12.5977\n",
      "Epoch [1/3], Step [7100/12942], Loss: 2.4381, Perplexity: 11.4513\n",
      "Epoch [1/3], Step [7200/12942], Loss: 2.4747, Perplexity: 11.8782\n",
      "Epoch [1/3], Step [7300/12942], Loss: 2.3910, Perplexity: 10.9245\n",
      "Epoch [1/3], Step [7400/12942], Loss: 2.3852, Perplexity: 10.8608\n",
      "Epoch [1/3], Step [7500/12942], Loss: 2.5736, Perplexity: 13.1125\n",
      "Epoch [1/3], Step [7600/12942], Loss: 2.2278, Perplexity: 9.27958\n",
      "Epoch [1/3], Step [7700/12942], Loss: 2.1896, Perplexity: 8.93205\n",
      "Epoch [1/3], Step [7800/12942], Loss: 2.5008, Perplexity: 12.1927\n",
      "Epoch [1/3], Step [7900/12942], Loss: 2.4720, Perplexity: 11.8465\n",
      "Epoch [1/3], Step [8000/12942], Loss: 2.3903, Perplexity: 10.9169\n",
      "Epoch [1/3], Step [8100/12942], Loss: 2.2055, Perplexity: 9.07514\n",
      "Epoch [1/3], Step [8200/12942], Loss: 2.4139, Perplexity: 11.1778\n",
      "Epoch [1/3], Step [8300/12942], Loss: 2.6162, Perplexity: 13.6839\n",
      "Epoch [1/3], Step [8400/12942], Loss: 2.1426, Perplexity: 8.52200\n",
      "Epoch [1/3], Step [8500/12942], Loss: 2.0920, Perplexity: 8.10137\n",
      "Epoch [1/3], Step [8600/12942], Loss: 2.5589, Perplexity: 12.9213\n",
      "Epoch [1/3], Step [8700/12942], Loss: 2.3883, Perplexity: 10.8945\n",
      "Epoch [1/3], Step [8800/12942], Loss: 3.0350, Perplexity: 20.8013\n",
      "Epoch [1/3], Step [8900/12942], Loss: 2.4589, Perplexity: 11.6920\n",
      "Epoch [1/3], Step [9000/12942], Loss: 2.2050, Perplexity: 9.06982\n",
      "Epoch [1/3], Step [9100/12942], Loss: 2.5423, Perplexity: 12.7090\n",
      "Epoch [1/3], Step [9200/12942], Loss: 2.4827, Perplexity: 11.9737\n",
      "Epoch [1/3], Step [9300/12942], Loss: 2.3193, Perplexity: 10.1687\n",
      "Epoch [1/3], Step [9400/12942], Loss: 2.0925, Perplexity: 8.10508\n",
      "Epoch [1/3], Step [9500/12942], Loss: 2.4753, Perplexity: 11.8848\n",
      "Epoch [1/3], Step [9600/12942], Loss: 2.3022, Perplexity: 9.99601\n",
      "Epoch [1/3], Step [9700/12942], Loss: 2.4793, Perplexity: 11.9333\n",
      "Epoch [1/3], Step [9800/12942], Loss: 2.2457, Perplexity: 9.44757\n",
      "Epoch [1/3], Step [9900/12942], Loss: 2.2122, Perplexity: 9.13607\n",
      "Epoch [1/3], Step [10000/12942], Loss: 2.5753, Perplexity: 13.1348\n",
      "Epoch [1/3], Step [10100/12942], Loss: 2.2571, Perplexity: 9.55544\n",
      "Epoch [1/3], Step [10200/12942], Loss: 2.2132, Perplexity: 9.14496\n",
      "Epoch [1/3], Step [10300/12942], Loss: 2.2451, Perplexity: 9.44150\n",
      "Epoch [1/3], Step [10400/12942], Loss: 2.4898, Perplexity: 12.0585\n",
      "Epoch [1/3], Step [10500/12942], Loss: 2.3517, Perplexity: 10.5033\n",
      "Epoch [1/3], Step [10600/12942], Loss: 2.2868, Perplexity: 9.84358\n",
      "Epoch [1/3], Step [10700/12942], Loss: 2.6044, Perplexity: 13.5229\n",
      "Epoch [1/3], Step [10800/12942], Loss: 2.4426, Perplexity: 11.5026\n",
      "Epoch [1/3], Step [10900/12942], Loss: 2.2811, Perplexity: 9.78779\n",
      "Epoch [1/3], Step [11000/12942], Loss: 2.1665, Perplexity: 8.72796\n",
      "Epoch [1/3], Step [11100/12942], Loss: 2.3174, Perplexity: 10.1492\n",
      "Epoch [1/3], Step [11200/12942], Loss: 2.2696, Perplexity: 9.67569\n",
      "Epoch [1/3], Step [11300/12942], Loss: 2.1891, Perplexity: 8.92762\n",
      "Epoch [1/3], Step [11400/12942], Loss: 2.0755, Perplexity: 7.96851\n",
      "Epoch [1/3], Step [11500/12942], Loss: 2.3281, Perplexity: 10.2584\n",
      "Epoch [1/3], Step [11600/12942], Loss: 2.3482, Perplexity: 10.4668\n",
      "Epoch [1/3], Step [11700/12942], Loss: 2.3924, Perplexity: 10.9397\n",
      "Epoch [1/3], Step [11800/12942], Loss: 2.2232, Perplexity: 9.23669\n",
      "Epoch [1/3], Step [11900/12942], Loss: 2.6201, Perplexity: 13.7370\n",
      "Epoch [1/3], Step [12000/12942], Loss: 2.1010, Perplexity: 8.17413\n",
      "Epoch [1/3], Step [12100/12942], Loss: 2.1444, Perplexity: 8.53730\n",
      "Epoch [1/3], Step [12200/12942], Loss: 2.2108, Perplexity: 9.12314\n",
      "Epoch [1/3], Step [12300/12942], Loss: 2.7369, Perplexity: 15.4384\n",
      "Epoch [1/3], Step [12400/12942], Loss: 2.2155, Perplexity: 9.16616\n",
      "Epoch [1/3], Step [12500/12942], Loss: 2.2864, Perplexity: 9.839488\n",
      "Epoch [1/3], Step [12600/12942], Loss: 2.2446, Perplexity: 9.43652\n",
      "Epoch [1/3], Step [12700/12942], Loss: 2.2231, Perplexity: 9.236319\n",
      "Epoch [1/3], Step [12800/12942], Loss: 2.0745, Perplexity: 7.96060\n",
      "Epoch [1/3], Step [12900/12942], Loss: 2.2754, Perplexity: 9.73236\n",
      "Epoch [2/3], Step [100/12942], Loss: 2.1765, Perplexity: 8.8154481\n",
      "Epoch [2/3], Step [200/12942], Loss: 2.2992, Perplexity: 9.96675\n",
      "Epoch [2/3], Step [300/12942], Loss: 2.8682, Perplexity: 17.6059\n",
      "Epoch [2/3], Step [400/12942], Loss: 2.0870, Perplexity: 8.06065\n",
      "Epoch [2/3], Step [500/12942], Loss: 2.1672, Perplexity: 8.73429\n",
      "Epoch [2/3], Step [600/12942], Loss: 2.0989, Perplexity: 8.15710\n",
      "Epoch [2/3], Step [700/12942], Loss: 2.5022, Perplexity: 12.2090\n",
      "Epoch [2/3], Step [800/12942], Loss: 2.6049, Perplexity: 13.5295\n",
      "Epoch [2/3], Step [900/12942], Loss: 2.0365, Perplexity: 7.66362\n",
      "Epoch [2/3], Step [1000/12942], Loss: 2.1338, Perplexity: 8.4465\n",
      "Epoch [2/3], Step [1100/12942], Loss: 2.6110, Perplexity: 13.6126\n",
      "Epoch [2/3], Step [1200/12942], Loss: 2.2189, Perplexity: 9.19746\n",
      "Epoch [2/3], Step [1300/12942], Loss: 2.2632, Perplexity: 9.61406\n",
      "Epoch [2/3], Step [1400/12942], Loss: 2.2307, Perplexity: 9.30664\n",
      "Epoch [2/3], Step [1500/12942], Loss: 2.4400, Perplexity: 11.4734\n",
      "Epoch [2/3], Step [1600/12942], Loss: 2.0656, Perplexity: 7.89037\n",
      "Epoch [2/3], Step [1700/12942], Loss: 2.3419, Perplexity: 10.4012\n",
      "Epoch [2/3], Step [1800/12942], Loss: 2.2984, Perplexity: 9.95807\n",
      "Epoch [2/3], Step [1900/12942], Loss: 2.2169, Perplexity: 9.17868\n",
      "Epoch [2/3], Step [2000/12942], Loss: 2.3183, Perplexity: 10.1580\n",
      "Epoch [2/3], Step [2100/12942], Loss: 2.4820, Perplexity: 11.9648\n",
      "Epoch [2/3], Step [2200/12942], Loss: 2.2211, Perplexity: 9.21793\n",
      "Epoch [2/3], Step [2300/12942], Loss: 1.7922, Perplexity: 6.00253\n",
      "Epoch [2/3], Step [2400/12942], Loss: 2.3834, Perplexity: 10.8412\n",
      "Epoch [2/3], Step [2500/12942], Loss: 2.0034, Perplexity: 7.41400\n",
      "Epoch [2/3], Step [2600/12942], Loss: 2.5819, Perplexity: 13.2218\n",
      "Epoch [2/3], Step [2700/12942], Loss: 2.3564, Perplexity: 10.5530\n",
      "Epoch [2/3], Step [2800/12942], Loss: 1.9418, Perplexity: 6.97138\n",
      "Epoch [2/3], Step [2900/12942], Loss: 2.1525, Perplexity: 8.60602\n",
      "Epoch [2/3], Step [3000/12942], Loss: 2.3205, Perplexity: 10.1812\n",
      "Epoch [2/3], Step [3100/12942], Loss: 2.0907, Perplexity: 8.09053\n",
      "Epoch [2/3], Step [3200/12942], Loss: 2.2104, Perplexity: 9.11901\n",
      "Epoch [2/3], Step [3300/12942], Loss: 2.3353, Perplexity: 10.3323\n",
      "Epoch [2/3], Step [3400/12942], Loss: 2.4760, Perplexity: 11.8934\n",
      "Epoch [2/3], Step [3500/12942], Loss: 2.1358, Perplexity: 8.46368\n",
      "Epoch [2/3], Step [3600/12942], Loss: 2.0288, Perplexity: 7.60529\n",
      "Epoch [2/3], Step [3700/12942], Loss: 2.0360, Perplexity: 7.66020\n",
      "Epoch [2/3], Step [3800/12942], Loss: 2.0885, Perplexity: 8.07292\n",
      "Epoch [2/3], Step [3900/12942], Loss: 2.3640, Perplexity: 10.6338\n",
      "Epoch [2/3], Step [4000/12942], Loss: 2.0616, Perplexity: 7.85867\n",
      "Epoch [2/3], Step [4100/12942], Loss: 2.8923, Perplexity: 18.03567\n",
      "Epoch [2/3], Step [4200/12942], Loss: 2.0411, Perplexity: 7.69903\n",
      "Epoch [2/3], Step [4300/12942], Loss: 2.2157, Perplexity: 9.16779\n",
      "Epoch [2/3], Step [4400/12942], Loss: 1.9957, Perplexity: 7.35730\n",
      "Epoch [2/3], Step [4500/12942], Loss: 2.2986, Perplexity: 9.96057\n",
      "Epoch [2/3], Step [4600/12942], Loss: 2.1774, Perplexity: 8.82375\n",
      "Epoch [2/3], Step [4700/12942], Loss: 1.9545, Perplexity: 7.06063\n",
      "Epoch [2/3], Step [4800/12942], Loss: 3.0241, Perplexity: 20.5751\n",
      "Epoch [2/3], Step [4900/12942], Loss: 2.1280, Perplexity: 8.39846\n",
      "Epoch [2/3], Step [5000/12942], Loss: 2.1176, Perplexity: 8.31092\n",
      "Epoch [2/3], Step [5100/12942], Loss: 2.2644, Perplexity: 9.62563\n",
      "Epoch [2/3], Step [5200/12942], Loss: 2.3994, Perplexity: 11.0169\n",
      "Epoch [2/3], Step [5300/12942], Loss: 2.1924, Perplexity: 8.95655\n",
      "Epoch [2/3], Step [5400/12942], Loss: 2.0348, Perplexity: 7.65050\n",
      "Epoch [2/3], Step [5500/12942], Loss: 1.9260, Perplexity: 6.86203\n",
      "Epoch [2/3], Step [5600/12942], Loss: 2.2419, Perplexity: 9.41098\n",
      "Epoch [2/3], Step [5700/12942], Loss: 2.0691, Perplexity: 7.91752\n",
      "Epoch [2/3], Step [5800/12942], Loss: 2.1897, Perplexity: 8.93232\n",
      "Epoch [2/3], Step [5900/12942], Loss: 2.1141, Perplexity: 8.28259\n",
      "Epoch [2/3], Step [6000/12942], Loss: 2.2334, Perplexity: 9.33188\n",
      "Epoch [2/3], Step [6100/12942], Loss: 2.3274, Perplexity: 10.2514\n",
      "Epoch [2/3], Step [6200/12942], Loss: 2.2714, Perplexity: 9.69315\n",
      "Epoch [2/3], Step [6300/12942], Loss: 2.1986, Perplexity: 9.01226\n",
      "Epoch [2/3], Step [6400/12942], Loss: 2.0969, Perplexity: 8.14088\n",
      "Epoch [2/3], Step [6500/12942], Loss: 2.1038, Perplexity: 8.19771\n",
      "Epoch [2/3], Step [6600/12942], Loss: 2.6778, Perplexity: 14.5525\n",
      "Epoch [2/3], Step [6700/12942], Loss: 2.4080, Perplexity: 11.1123\n",
      "Epoch [2/3], Step [6800/12942], Loss: 2.4765, Perplexity: 11.8992\n",
      "Epoch [2/3], Step [6900/12942], Loss: 2.6137, Perplexity: 13.6499\n",
      "Epoch [2/3], Step [7000/12942], Loss: 2.1706, Perplexity: 8.76345\n",
      "Epoch [2/3], Step [7100/12942], Loss: 2.6123, Perplexity: 13.6302\n",
      "Epoch [2/3], Step [7200/12942], Loss: 2.4691, Perplexity: 11.8116\n",
      "Epoch [2/3], Step [7300/12942], Loss: 2.3625, Perplexity: 10.6172\n",
      "Epoch [2/3], Step [7400/12942], Loss: 2.2271, Perplexity: 9.27293\n",
      "Epoch [2/3], Step [7500/12942], Loss: 2.0321, Perplexity: 7.63009\n",
      "Epoch [2/3], Step [7600/12942], Loss: 2.3519, Perplexity: 10.5059\n",
      "Epoch [2/3], Step [7700/12942], Loss: 2.1276, Perplexity: 8.39470\n",
      "Epoch [2/3], Step [7800/12942], Loss: 2.5533, Perplexity: 12.8489\n",
      "Epoch [2/3], Step [7900/12942], Loss: 2.1769, Perplexity: 8.81860\n",
      "Epoch [2/3], Step [8000/12942], Loss: 2.0456, Perplexity: 7.73406\n",
      "Epoch [2/3], Step [8100/12942], Loss: 2.1478, Perplexity: 8.56601\n",
      "Epoch [2/3], Step [8200/12942], Loss: 2.1781, Perplexity: 8.82983\n",
      "Epoch [2/3], Step [8300/12942], Loss: 1.9871, Perplexity: 7.29430\n",
      "Epoch [2/3], Step [8400/12942], Loss: 2.2500, Perplexity: 9.48756\n",
      "Epoch [2/3], Step [8500/12942], Loss: 2.0012, Perplexity: 7.39826\n",
      "Epoch [2/3], Step [8600/12942], Loss: 1.8345, Perplexity: 6.26196\n",
      "Epoch [2/3], Step [8700/12942], Loss: 2.2467, Perplexity: 9.45655\n",
      "Epoch [2/3], Step [8800/12942], Loss: 2.3976, Perplexity: 10.9970\n",
      "Epoch [2/3], Step [8900/12942], Loss: 1.9129, Perplexity: 6.77286\n",
      "Epoch [2/3], Step [9000/12942], Loss: 2.0015, Perplexity: 7.40006\n",
      "Epoch [2/3], Step [9100/12942], Loss: 1.9971, Perplexity: 7.36744\n",
      "Epoch [2/3], Step [9200/12942], Loss: 2.0803, Perplexity: 8.00716\n",
      "Epoch [2/3], Step [9300/12942], Loss: 2.2614, Perplexity: 9.59623\n",
      "Epoch [2/3], Step [9400/12942], Loss: 2.2115, Perplexity: 9.12914\n",
      "Epoch [2/3], Step [9500/12942], Loss: 2.3350, Perplexity: 10.3296\n",
      "Epoch [2/3], Step [9600/12942], Loss: 2.4526, Perplexity: 11.6180\n",
      "Epoch [2/3], Step [9700/12942], Loss: 2.1134, Perplexity: 8.27660\n",
      "Epoch [2/3], Step [9800/12942], Loss: 2.0837, Perplexity: 8.03420\n",
      "Epoch [2/3], Step [9900/12942], Loss: 2.0714, Perplexity: 7.93624\n",
      "Epoch [2/3], Step [10000/12942], Loss: 2.0414, Perplexity: 7.7012\n",
      "Epoch [2/3], Step [10100/12942], Loss: 1.9167, Perplexity: 6.79821\n",
      "Epoch [2/3], Step [10200/12942], Loss: 1.9751, Perplexity: 7.20704\n",
      "Epoch [2/3], Step [10300/12942], Loss: 2.3317, Perplexity: 10.2957\n",
      "Epoch [2/3], Step [10400/12942], Loss: 2.1522, Perplexity: 8.60339\n",
      "Epoch [2/3], Step [10500/12942], Loss: 2.0442, Perplexity: 7.72327\n",
      "Epoch [2/3], Step [10600/12942], Loss: 2.1581, Perplexity: 8.65492\n",
      "Epoch [2/3], Step [10700/12942], Loss: 2.1849, Perplexity: 8.89009\n",
      "Epoch [2/3], Step [10800/12942], Loss: 1.9873, Perplexity: 7.29604\n",
      "Epoch [2/3], Step [10900/12942], Loss: 2.2332, Perplexity: 9.32984\n",
      "Epoch [2/3], Step [11000/12942], Loss: 2.9814, Perplexity: 19.7159\n",
      "Epoch [2/3], Step [11100/12942], Loss: 2.1548, Perplexity: 8.62617\n",
      "Epoch [2/3], Step [11200/12942], Loss: 1.9897, Perplexity: 7.31336\n",
      "Epoch [2/3], Step [11300/12942], Loss: 2.0100, Perplexity: 7.46305\n",
      "Epoch [2/3], Step [11400/12942], Loss: 1.9568, Perplexity: 7.07687\n",
      "Epoch [2/3], Step [11500/12942], Loss: 2.1283, Perplexity: 8.40083\n",
      "Epoch [2/3], Step [11600/12942], Loss: 1.8958, Perplexity: 6.65793\n",
      "Epoch [2/3], Step [11700/12942], Loss: 2.2562, Perplexity: 9.54697\n",
      "Epoch [2/3], Step [11800/12942], Loss: 2.2653, Perplexity: 9.63364\n",
      "Epoch [2/3], Step [11900/12942], Loss: 2.1297, Perplexity: 8.41278\n",
      "Epoch [2/3], Step [12000/12942], Loss: 2.0240, Perplexity: 7.56873\n",
      "Epoch [2/3], Step [12100/12942], Loss: 1.9088, Perplexity: 6.74487\n",
      "Epoch [2/3], Step [12200/12942], Loss: 1.9349, Perplexity: 6.92339\n",
      "Epoch [2/3], Step [12300/12942], Loss: 2.1333, Perplexity: 8.44302\n",
      "Epoch [2/3], Step [12400/12942], Loss: 2.0654, Perplexity: 7.88818\n",
      "Epoch [2/3], Step [12500/12942], Loss: 2.0507, Perplexity: 7.77369\n",
      "Epoch [2/3], Step [12600/12942], Loss: 2.2343, Perplexity: 9.33967\n",
      "Epoch [2/3], Step [12700/12942], Loss: 2.1942, Perplexity: 8.97308\n",
      "Epoch [2/3], Step [12800/12942], Loss: 2.0018, Perplexity: 7.40224\n",
      "Epoch [2/3], Step [12900/12942], Loss: 1.9206, Perplexity: 6.82535\n",
      "Epoch [3/3], Step [100/12942], Loss: 2.0567, Perplexity: 7.8202231\n",
      "Epoch [3/3], Step [200/12942], Loss: 2.0916, Perplexity: 8.09757\n",
      "Epoch [3/3], Step [300/12942], Loss: 2.1947, Perplexity: 8.97739\n",
      "Epoch [3/3], Step [400/12942], Loss: 2.7273, Perplexity: 15.2916\n",
      "Epoch [3/3], Step [500/12942], Loss: 2.0878, Perplexity: 8.06722\n",
      "Epoch [3/3], Step [600/12942], Loss: 2.7423, Perplexity: 15.5228\n",
      "Epoch [3/3], Step [700/12942], Loss: 2.2145, Perplexity: 9.15699\n",
      "Epoch [3/3], Step [800/12942], Loss: 2.2361, Perplexity: 9.35718\n",
      "Epoch [3/3], Step [900/12942], Loss: 2.2851, Perplexity: 9.82703\n",
      "Epoch [3/3], Step [1000/12942], Loss: 1.8644, Perplexity: 6.4522\n",
      "Epoch [3/3], Step [1100/12942], Loss: 2.1167, Perplexity: 8.30335\n",
      "Epoch [3/3], Step [1200/12942], Loss: 2.2415, Perplexity: 9.40769\n",
      "Epoch [3/3], Step [1300/12942], Loss: 2.0820, Perplexity: 8.02038\n",
      "Epoch [3/3], Step [1400/12942], Loss: 2.1044, Perplexity: 8.20194\n",
      "Epoch [3/3], Step [1500/12942], Loss: 2.5562, Perplexity: 12.8863\n",
      "Epoch [3/3], Step [1600/12942], Loss: 2.3265, Perplexity: 10.2417\n",
      "Epoch [3/3], Step [1700/12942], Loss: 2.4714, Perplexity: 11.8384\n",
      "Epoch [3/3], Step [1800/12942], Loss: 1.7215, Perplexity: 5.59272\n",
      "Epoch [3/3], Step [1900/12942], Loss: 2.3513, Perplexity: 10.4996\n",
      "Epoch [3/3], Step [2000/12942], Loss: 2.0818, Perplexity: 8.01911\n",
      "Epoch [3/3], Step [2100/12942], Loss: 2.1213, Perplexity: 8.34226\n",
      "Epoch [3/3], Step [2200/12942], Loss: 2.4541, Perplexity: 11.6365\n",
      "Epoch [3/3], Step [2300/12942], Loss: 1.9032, Perplexity: 6.70703\n",
      "Epoch [3/3], Step [2400/12942], Loss: 2.0346, Perplexity: 7.64928\n",
      "Epoch [3/3], Step [2500/12942], Loss: 2.0746, Perplexity: 7.96171\n",
      "Epoch [3/3], Step [2600/12942], Loss: 2.1471, Perplexity: 8.55972\n",
      "Epoch [3/3], Step [2700/12942], Loss: 2.0682, Perplexity: 7.91059\n",
      "Epoch [3/3], Step [2800/12942], Loss: 2.1289, Perplexity: 8.40587\n",
      "Epoch [3/3], Step [2900/12942], Loss: 2.0128, Perplexity: 7.48457\n",
      "Epoch [3/3], Step [3000/12942], Loss: 1.8781, Perplexity: 6.54089\n",
      "Epoch [3/3], Step [3100/12942], Loss: 2.9972, Perplexity: 20.0285\n",
      "Epoch [3/3], Step [3200/12942], Loss: 2.1546, Perplexity: 8.62454\n",
      "Epoch [3/3], Step [3300/12942], Loss: 2.2181, Perplexity: 9.19025\n",
      "Epoch [3/3], Step [3400/12942], Loss: 2.3747, Perplexity: 10.7476\n",
      "Epoch [3/3], Step [3500/12942], Loss: 2.1255, Perplexity: 8.37731\n",
      "Epoch [3/3], Step [3600/12942], Loss: 1.8836, Perplexity: 6.57705\n",
      "Epoch [3/3], Step [3700/12942], Loss: 2.1491, Perplexity: 8.57701\n",
      "Epoch [3/3], Step [3800/12942], Loss: 2.2022, Perplexity: 9.04457\n",
      "Epoch [3/3], Step [3900/12942], Loss: 2.1455, Perplexity: 8.54670\n",
      "Epoch [3/3], Step [4000/12942], Loss: 2.6017, Perplexity: 13.4861\n",
      "Epoch [3/3], Step [4100/12942], Loss: 1.9611, Perplexity: 7.10756\n",
      "Epoch [3/3], Step [4200/12942], Loss: 1.9558, Perplexity: 7.06963\n",
      "Epoch [3/3], Step [4300/12942], Loss: 2.1063, Perplexity: 8.21798\n",
      "Epoch [3/3], Step [4400/12942], Loss: 2.1137, Perplexity: 8.27895\n",
      "Epoch [3/3], Step [4500/12942], Loss: 2.1058, Perplexity: 8.21359\n",
      "Epoch [3/3], Step [4600/12942], Loss: 2.1630, Perplexity: 8.69748\n",
      "Epoch [3/3], Step [4700/12942], Loss: 2.4305, Perplexity: 11.3651\n",
      "Epoch [3/3], Step [4800/12942], Loss: 2.4060, Perplexity: 11.0891\n",
      "Epoch [3/3], Step [4900/12942], Loss: 1.9727, Perplexity: 7.19020\n",
      "Epoch [3/3], Step [5000/12942], Loss: 2.1650, Perplexity: 8.71440\n",
      "Epoch [3/3], Step [5100/12942], Loss: 2.2714, Perplexity: 9.69272\n",
      "Epoch [3/3], Step [5200/12942], Loss: 2.0076, Perplexity: 7.44545\n",
      "Epoch [3/3], Step [5300/12942], Loss: 1.7696, Perplexity: 5.86859\n",
      "Epoch [3/3], Step [5400/12942], Loss: 2.1447, Perplexity: 8.53930\n",
      "Epoch [3/3], Step [5500/12942], Loss: 1.9039, Perplexity: 6.71210\n",
      "Epoch [3/3], Step [5600/12942], Loss: 2.2788, Perplexity: 9.76517\n",
      "Epoch [3/3], Step [5700/12942], Loss: 2.2052, Perplexity: 9.07206\n",
      "Epoch [3/3], Step [5800/12942], Loss: 1.9381, Perplexity: 6.94545\n",
      "Epoch [3/3], Step [5900/12942], Loss: 2.3976, Perplexity: 10.9964\n",
      "Epoch [3/3], Step [6000/12942], Loss: 2.3539, Perplexity: 10.5268\n",
      "Epoch [3/3], Step [6100/12942], Loss: 2.2105, Perplexity: 9.12071\n",
      "Epoch [3/3], Step [6200/12942], Loss: 1.8834, Perplexity: 6.57579\n",
      "Epoch [3/3], Step [6300/12942], Loss: 2.3449, Perplexity: 10.4318\n",
      "Epoch [3/3], Step [6400/12942], Loss: 2.4215, Perplexity: 11.2629\n",
      "Epoch [3/3], Step [6500/12942], Loss: 2.3322, Perplexity: 10.3002\n",
      "Epoch [3/3], Step [6600/12942], Loss: 2.0894, Perplexity: 8.08014\n",
      "Epoch [3/3], Step [6700/12942], Loss: 1.7612, Perplexity: 5.81963\n",
      "Epoch [3/3], Step [6800/12942], Loss: 2.0847, Perplexity: 8.04236\n",
      "Epoch [3/3], Step [6900/12942], Loss: 2.3315, Perplexity: 10.2933\n",
      "Epoch [3/3], Step [7000/12942], Loss: 2.3569, Perplexity: 10.5581\n",
      "Epoch [3/3], Step [7100/12942], Loss: 2.5316, Perplexity: 12.5741\n",
      "Epoch [3/3], Step [7200/12942], Loss: 1.9614, Perplexity: 7.10948\n",
      "Epoch [3/3], Step [7300/12942], Loss: 2.1550, Perplexity: 8.62837\n",
      "Epoch [3/3], Step [7400/12942], Loss: 2.0356, Perplexity: 7.65690\n",
      "Epoch [3/3], Step [7500/12942], Loss: 1.9914, Perplexity: 7.32594\n",
      "Epoch [3/3], Step [7600/12942], Loss: 1.8258, Perplexity: 6.20773\n",
      "Epoch [3/3], Step [7700/12942], Loss: 2.0567, Perplexity: 7.82024\n",
      "Epoch [3/3], Step [7800/12942], Loss: 2.2022, Perplexity: 9.04469\n",
      "Epoch [3/3], Step [7900/12942], Loss: 2.1306, Perplexity: 8.41987\n",
      "Epoch [3/3], Step [8000/12942], Loss: 2.3280, Perplexity: 10.2577\n",
      "Epoch [3/3], Step [8100/12942], Loss: 2.2143, Perplexity: 9.15461\n",
      "Epoch [3/3], Step [8200/12942], Loss: 1.9913, Perplexity: 7.32536\n",
      "Epoch [3/3], Step [8300/12942], Loss: 2.5302, Perplexity: 12.5561\n",
      "Epoch [3/3], Step [8400/12942], Loss: 2.0434, Perplexity: 7.71660\n",
      "Epoch [3/3], Step [8500/12942], Loss: 2.2525, Perplexity: 9.51120\n",
      "Epoch [3/3], Step [8600/12942], Loss: 2.1362, Perplexity: 8.46754\n",
      "Epoch [3/3], Step [8700/12942], Loss: 2.8023, Perplexity: 16.4825\n",
      "Epoch [3/3], Step [8800/12942], Loss: 1.8454, Perplexity: 6.33092\n",
      "Epoch [3/3], Step [8900/12942], Loss: 1.8223, Perplexity: 6.18608\n",
      "Epoch [3/3], Step [9000/12942], Loss: 1.8490, Perplexity: 6.35345\n",
      "Epoch [3/3], Step [9100/12942], Loss: 2.2291, Perplexity: 9.29167\n",
      "Epoch [3/3], Step [9200/12942], Loss: 2.1548, Perplexity: 8.62644\n",
      "Epoch [3/3], Step [9300/12942], Loss: 3.1407, Perplexity: 23.1197\n",
      "Epoch [3/3], Step [9400/12942], Loss: 2.0718, Perplexity: 7.93912\n",
      "Epoch [3/3], Step [9500/12942], Loss: 1.9115, Perplexity: 6.76329\n",
      "Epoch [3/3], Step [9600/12942], Loss: 1.8474, Perplexity: 6.34347\n",
      "Epoch [3/3], Step [9700/12942], Loss: 2.1598, Perplexity: 8.66970\n",
      "Epoch [3/3], Step [9800/12942], Loss: 2.0175, Perplexity: 7.51926\n",
      "Epoch [3/3], Step [9900/12942], Loss: 2.1467, Perplexity: 8.55661\n",
      "Epoch [3/3], Step [10000/12942], Loss: 2.3428, Perplexity: 10.4099\n",
      "Epoch [3/3], Step [10100/12942], Loss: 1.9804, Perplexity: 7.24581\n",
      "Epoch [3/3], Step [10200/12942], Loss: 2.1934, Perplexity: 8.96592\n",
      "Epoch [3/3], Step [10300/12942], Loss: 2.0669, Perplexity: 7.90032\n",
      "Epoch [3/3], Step [10400/12942], Loss: 2.4313, Perplexity: 11.3737\n",
      "Epoch [3/3], Step [10500/12942], Loss: 2.2176, Perplexity: 9.18534\n",
      "Epoch [3/3], Step [10600/12942], Loss: 2.1555, Perplexity: 8.63231\n",
      "Epoch [3/3], Step [10700/12942], Loss: 2.0827, Perplexity: 8.02606\n",
      "Epoch [3/3], Step [10800/12942], Loss: 2.1010, Perplexity: 8.17459\n",
      "Epoch [3/3], Step [10900/12942], Loss: 1.9897, Perplexity: 7.31318\n",
      "Epoch [3/3], Step [11000/12942], Loss: 1.9797, Perplexity: 7.24074\n",
      "Epoch [3/3], Step [11100/12942], Loss: 1.8364, Perplexity: 6.27410\n",
      "Epoch [3/3], Step [11200/12942], Loss: 2.0816, Perplexity: 8.01704\n",
      "Epoch [3/3], Step [11300/12942], Loss: 1.9250, Perplexity: 6.85530\n",
      "Epoch [3/3], Step [11400/12942], Loss: 2.3058, Perplexity: 10.03255\n",
      "Epoch [3/3], Step [11500/12942], Loss: 2.1121, Perplexity: 8.26594\n",
      "Epoch [3/3], Step [11600/12942], Loss: 1.9352, Perplexity: 6.92558\n",
      "Epoch [3/3], Step [11700/12942], Loss: 2.9333, Perplexity: 18.7888\n",
      "Epoch [3/3], Step [11800/12942], Loss: 2.0934, Perplexity: 8.11279\n",
      "Epoch [3/3], Step [11900/12942], Loss: 2.3574, Perplexity: 10.5632\n",
      "Epoch [3/3], Step [12000/12942], Loss: 1.8244, Perplexity: 6.19902\n",
      "Epoch [3/3], Step [12100/12942], Loss: 2.2342, Perplexity: 9.33877\n",
      "Epoch [3/3], Step [12200/12942], Loss: 2.2375, Perplexity: 9.36984\n",
      "Epoch [3/3], Step [12300/12942], Loss: 2.0695, Perplexity: 7.92060\n",
      "Epoch [3/3], Step [12400/12942], Loss: 2.1823, Perplexity: 8.86692\n",
      "Epoch [3/3], Step [12500/12942], Loss: 2.1379, Perplexity: 8.48153\n",
      "Epoch [3/3], Step [12600/12942], Loss: 2.3883, Perplexity: 10.8950\n",
      "Epoch [3/3], Step [12700/12942], Loss: 2.1332, Perplexity: 8.44218\n",
      "Epoch [3/3], Step [12800/12942], Loss: 2.0410, Perplexity: 7.69848\n",
      "Epoch [3/3], Step [12900/12942], Loss: 2.0125, Perplexity: 7.48173\n",
      "Epoch [3/3], Step [12942/12942], Loss: 2.1450, Perplexity: 8.54184"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "response = requests.request(\"GET\", \n",
    "                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "            requests.request(\"POST\", \n",
    "                             \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "                             headers={'Authorization': \"STAR \" + response.text})\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
